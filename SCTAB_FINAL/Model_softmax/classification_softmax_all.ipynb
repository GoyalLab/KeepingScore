{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_PATH = \"/projects/b1042/GoyalLab/jaekj/SCTAB_FINAL/Model_softmax/Sample_all_T_1000_Path_2/class_results\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Aggregated results from 164 classes.\n",
      "[INFO] Total samples: 3448832\n"
     ]
    }
   ],
   "source": [
    "y_true_all = []\n",
    "y_pred_all = []\n",
    "posteriors_all = []\n",
    "\n",
    "for class_dir in sorted(os.listdir(SAVE_PATH)):\n",
    "    if not class_dir.startswith(\"class_\"):\n",
    "        continue\n",
    "    class_path = os.path.join(SAVE_PATH, class_dir)\n",
    "    y_true = np.load(os.path.join(class_path, \"y_true.npy\"))\n",
    "    y_pred = np.load(os.path.join(class_path, \"y_pred.npy\"))\n",
    "    posteriors = np.load(os.path.join(class_path, \"posteriors.npy\"), allow_pickle=True)\n",
    "\n",
    "    y_true_all.append(y_true)\n",
    "    y_pred_all.append(y_pred)\n",
    "    posteriors_all.extend(posteriors)\n",
    "\n",
    "y_true_all = np.concatenate(y_true_all)\n",
    "y_pred_all = np.concatenate(y_pred_all)\n",
    "\n",
    "print(f\"[INFO] Aggregated results from {len(os.listdir(SAVE_PATH))} classes.\")\n",
    "print(f\"[INFO] Total samples: {len(y_true_all)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RESULT] Softmax posterior inference — Macro F1: 0.805\n"
     ]
    }
   ],
   "source": [
    "f1_softmax = f1_score(y_true_all, y_pred_all, average=\"macro\")\n",
    "print(f\"[RESULT] Softmax posterior inference — Macro F1: {f1_softmax:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.937     0.932     0.934    156566\n",
      "           1      0.970     0.882     0.924      4155\n",
      "           2      0.937     0.967     0.952      8611\n",
      "           3      0.943     0.942     0.943     53049\n",
      "           4      0.939     0.966     0.952     34856\n",
      "           5      0.354     0.572     0.437      1240\n",
      "           6      0.621     0.882     0.729      5495\n",
      "           7      0.931     0.923     0.927     43004\n",
      "           8      0.802     0.932     0.862     10002\n",
      "           9      0.377     0.462     0.415     16651\n",
      "          10      0.908     0.810     0.856    194454\n",
      "          11      0.585     0.672     0.626      4696\n",
      "          12      0.635     0.903     0.746     30282\n",
      "          13      0.718     0.856     0.781      1307\n",
      "          14      0.881     0.781     0.828    202852\n",
      "          15      0.686     0.820     0.747     19590\n",
      "          16      0.599     0.785     0.679     23540\n",
      "          17      0.887     0.906     0.897     12737\n",
      "          18      0.592     0.414     0.487      1730\n",
      "          19      0.992     0.998     0.995     54683\n",
      "          20      0.981     0.991     0.986      1256\n",
      "          21      0.724     0.872     0.791      1916\n",
      "          22      0.832     0.876     0.853     97870\n",
      "          23      0.478     0.540     0.507      6381\n",
      "          24      0.626     0.486     0.547      6447\n",
      "          25      0.476     0.741     0.580      2528\n",
      "          26      0.703     0.532     0.605      6652\n",
      "          27      0.322     0.524     0.399      4471\n",
      "          28      0.531     0.592     0.560      4580\n",
      "          29      0.777     0.902     0.835      1413\n",
      "          30      0.880     0.950     0.913     37373\n",
      "          31      0.958     0.955     0.956      1144\n",
      "          32      0.890     0.935     0.912      1407\n",
      "          33      0.988     0.971     0.979      9604\n",
      "          34      0.965     0.979     0.972     15059\n",
      "          35      0.989     0.995     0.992      6843\n",
      "          36      0.888     0.851     0.869     35058\n",
      "          37      0.400     0.910     0.556      1495\n",
      "          38      0.903     0.977     0.939     16909\n",
      "          39      0.978     0.936     0.956      3532\n",
      "          40      0.766     0.724     0.745     18265\n",
      "          41      0.986     0.986     0.986     44531\n",
      "          42      0.986     0.940     0.962      1308\n",
      "          43      0.960     0.975     0.968       646\n",
      "          44      0.861     0.712     0.780     61351\n",
      "          45      0.490     0.536     0.512      4286\n",
      "          46      0.963     0.996     0.979      9560\n",
      "          47      0.988     0.991     0.989       900\n",
      "          48      0.988     0.981     0.984      8223\n",
      "          49      0.486     0.800     0.605      2146\n",
      "          50      0.894     0.891     0.893    145531\n",
      "          51      0.825     0.734     0.777      9573\n",
      "          52      0.787     0.920     0.848     10998\n",
      "          53      0.994     0.991     0.992      1268\n",
      "          54      0.698     0.761     0.728     11536\n",
      "          55      0.282     0.556     0.374       505\n",
      "          56      0.933     0.805     0.864      8518\n",
      "          57      0.953     0.963     0.958     25865\n",
      "          58      0.734     0.578     0.647     14519\n",
      "          59      0.615     0.729     0.667      9783\n",
      "          60      0.846     0.742     0.790     23296\n",
      "          61      0.506     0.703     0.589      2040\n",
      "          62      0.850     0.960     0.902      6706\n",
      "          63      0.853     0.863     0.858     58812\n",
      "          64      0.694     0.732     0.712      4543\n",
      "          65      0.872     0.924     0.898      7545\n",
      "          66      0.921     0.938     0.929      5669\n",
      "          67      0.961     0.980     0.970     30853\n",
      "          68      0.939     0.839     0.886      1109\n",
      "          69      0.875     0.991     0.929       451\n",
      "          70      0.970     0.976     0.973     33468\n",
      "          71      0.280     0.644     0.390       146\n",
      "          72      0.845     0.679     0.753      9733\n",
      "          73      0.517     0.901     0.657      3135\n",
      "          74      0.991     0.990     0.990     55675\n",
      "          75      0.986     0.993     0.989     42533\n",
      "          76      0.970     0.970     0.970      6437\n",
      "          77      0.719     0.655     0.686     19245\n",
      "          78      0.978     0.977     0.977     61195\n",
      "          79      0.927     0.884     0.905      6604\n",
      "          80      0.918     0.583     0.713      1675\n",
      "          81      0.957     0.973     0.965      3829\n",
      "          82      0.488     0.471     0.480      2389\n",
      "          83      0.586     0.550     0.568      1800\n",
      "          84      0.955     0.944     0.949      6222\n",
      "          85      0.868     0.930     0.898      1505\n",
      "          86      0.737     0.151     0.251      5560\n",
      "          87      0.370     0.311     0.338      1401\n",
      "          88      0.886     0.835     0.859      1362\n",
      "          89      0.296     0.915     0.447       340\n",
      "          90      0.797     0.144     0.244      4436\n",
      "          91      0.946     0.927     0.937      3196\n",
      "          92      0.816     0.845     0.830      3980\n",
      "          93      0.777     0.812     0.794      2476\n",
      "          94      0.871     0.905     0.888      3739\n",
      "          95      0.924     0.895     0.910      1022\n",
      "          96      0.911     0.913     0.912     14973\n",
      "          97      0.705     0.770     0.736      1575\n",
      "          98      0.798     0.776     0.787       648\n",
      "          99      0.986     0.990     0.988      5171\n",
      "         100      0.861     0.666     0.751      4873\n",
      "         101      0.979     0.995     0.987     39413\n",
      "         102      0.834     0.749     0.789      1078\n",
      "         103      0.940     0.955     0.948       852\n",
      "         104      0.771     0.851     0.809      5413\n",
      "         105      0.904     0.848     0.875      1797\n",
      "         106      0.852     0.703     0.770     77843\n",
      "         107      0.889     0.900     0.894     12593\n",
      "         108      0.731     0.691     0.710       640\n",
      "         109      0.592     0.660     0.624     10108\n",
      "         110      0.980     0.957     0.968      8614\n",
      "         111      0.852     0.985     0.913     43441\n",
      "         112      0.390     0.786     0.521      1894\n",
      "         113      0.457     0.344     0.392       818\n",
      "         114      0.800     0.762     0.780     39750\n",
      "         115      0.694     0.080     0.143      2559\n",
      "         116      0.971     0.981     0.976      5202\n",
      "         117      0.741     0.950     0.833     28361\n",
      "         118      0.872     0.814     0.842     92975\n",
      "         119      0.746     0.818     0.780      9414\n",
      "         120      0.880     0.910     0.894      1039\n",
      "         121      0.955     0.956     0.956      3375\n",
      "         122      0.861     0.819     0.840     39991\n",
      "         123      0.198     0.681     0.307      1748\n",
      "         124      0.811     0.863     0.836     95802\n",
      "         125      0.851     0.855     0.853     33747\n",
      "         126      0.901     0.870     0.885      5365\n",
      "         127      0.894     0.900     0.897    115901\n",
      "         128      0.965     0.996     0.980      1367\n",
      "         129      0.999     0.985     0.992    344083\n",
      "         130      0.894     0.793     0.840     18203\n",
      "         131      0.731     0.924     0.816     27659\n",
      "         132      0.998     0.987     0.992     82459\n",
      "         133      0.856     0.981     0.914     24793\n",
      "         134      0.961     0.948     0.954      1299\n",
      "         135      0.868     0.849     0.858     17812\n",
      "         136      0.879     0.917     0.898     36364\n",
      "         137      0.738     0.900     0.811      5372\n",
      "         138      0.872     0.861     0.866      8879\n",
      "         139      0.802     0.888     0.843      7903\n",
      "         140      0.803     0.828     0.815      4796\n",
      "         141      0.850     0.885     0.867       710\n",
      "         142      0.644     0.947     0.767      3069\n",
      "         143      0.683     0.818     0.744      1010\n",
      "         144      0.984     0.990     0.987      6572\n",
      "         145      0.745     0.764     0.754     25125\n",
      "         146      0.943     0.861     0.900       883\n",
      "         147      0.924     0.895     0.909     14989\n",
      "         148      0.224     0.573     0.322       506\n",
      "         149      0.979     0.963     0.971      2864\n",
      "         150      0.962     0.837     0.895      1868\n",
      "         151      0.960     0.780     0.861      3881\n",
      "         152      0.976     0.983     0.980     45986\n",
      "         153      0.713     0.832     0.768      9361\n",
      "         154      0.962     0.967     0.965      1729\n",
      "         155      0.942     0.988     0.965      3769\n",
      "         156      0.924     0.954     0.939      9820\n",
      "         157      0.936     0.934     0.935      2965\n",
      "         158      0.605     0.923     0.731      4532\n",
      "         159      0.861     0.955     0.906      4525\n",
      "         160      0.952     0.977     0.964     27258\n",
      "         161      0.914     0.822     0.866     11491\n",
      "         162      0.801     0.766     0.783     14240\n",
      "         163      0.981     0.995     0.988      8840\n",
      "\n",
      "    accuracy                          0.880   3448832\n",
      "   macro avg      0.804     0.830     0.805   3448832\n",
      "weighted avg      0.886     0.880     0.880   3448832\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_true_all, y_pred_all, digits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.879529</td>\n",
       "      <td>0.879529</td>\n",
       "      <td>0.879529</td>\n",
       "      <td>8.795288e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macro avg</th>\n",
       "      <td>0.804279</td>\n",
       "      <td>0.829766</td>\n",
       "      <td>0.805206</td>\n",
       "      <td>3.448832e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weighted avg</th>\n",
       "      <td>0.886213</td>\n",
       "      <td>0.879529</td>\n",
       "      <td>0.879895</td>\n",
       "      <td>3.448832e+06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              precision    recall  f1-score       support\n",
       "accuracy       0.879529  0.879529  0.879529  8.795288e-01\n",
       "macro avg      0.804279  0.829766  0.805206  3.448832e+06\n",
       "weighted avg   0.886213  0.879529  0.879895  3.448832e+06"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import pandas as pd\n",
    "\n",
    "clf_report = pd.DataFrame(classification_report(y_true_all, y_pred_all, output_dict=True)).T\n",
    "clf_report_overall = clf_report.iloc[-3:].copy()\n",
    "clf_report_per_class = clf_report.iloc[:-3].copy()\n",
    "\n",
    "clf_report_overall"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
