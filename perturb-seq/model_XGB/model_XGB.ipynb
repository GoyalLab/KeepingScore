{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/projects/b1042/GoyalLab/jaekj/python/DL_py3.10/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# === Train Conditional Diffusion Model on VAE Embeddings with Classifier-Based Guidance ===\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import scanpy as sc\n",
    "import scipy.sparse as sp\n",
    "import pandas as pd\n",
    "import anndata as ad\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "import umap\n",
    "import numpy as np\n",
    "import math\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Ground truth ===\n",
    "import os\n",
    "data_dir = \"/projects/b1042/GoyalLab/jaekj/perturb-seq\"\n",
    "z_train = np.load(os.path.join(data_dir, \"z_train.npy\"))\n",
    "z_val = np.load(os.path.join(data_dir, \"z_val.npy\"))\n",
    "z_test = np.load(os.path.join(data_dir, \"z_test.npy\")) \n",
    "\n",
    "y_train = np.load(os.path.join(data_dir, \"y_train.npy\")).astype(int)\n",
    "y_val = np.load(os.path.join(data_dir, \"y_val.npy\")).astype(int)\n",
    "y_test = np.load(os.path.join(data_dir, \"y_test.npy\")).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5, 19, 68, ...,  6, 16, 63], shape=(8477,))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_torch = torch.from_numpy(y_test)\n",
    "unique_labels = torch.unique(y_test_torch)\n",
    "unique_test_labels = [int(v) for v in unique_labels.numpy()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 10,\n",
       " 11,\n",
       " 12,\n",
       " 13,\n",
       " 14,\n",
       " 15,\n",
       " 16,\n",
       " 17,\n",
       " 18,\n",
       " 19,\n",
       " 20,\n",
       " 21,\n",
       " 22,\n",
       " 23,\n",
       " 24,\n",
       " 25,\n",
       " 26,\n",
       " 27,\n",
       " 28,\n",
       " 29,\n",
       " 30,\n",
       " 31,\n",
       " 32,\n",
       " 33,\n",
       " 34,\n",
       " 35,\n",
       " 36,\n",
       " 37,\n",
       " 38,\n",
       " 39,\n",
       " 40,\n",
       " 41,\n",
       " 42,\n",
       " 43,\n",
       " 44,\n",
       " 45,\n",
       " 46,\n",
       " 47,\n",
       " 48,\n",
       " 49,\n",
       " 50,\n",
       " 51,\n",
       " 52,\n",
       " 53,\n",
       " 54,\n",
       " 55,\n",
       " 56,\n",
       " 57,\n",
       " 58,\n",
       " 59,\n",
       " 60,\n",
       " 61,\n",
       " 62,\n",
       " 63,\n",
       " 64,\n",
       " 65,\n",
       " 66,\n",
       " 67,\n",
       " 68,\n",
       " 69,\n",
       " 70,\n",
       " 71,\n",
       " 72,\n",
       " 73,\n",
       " 74,\n",
       " 75,\n",
       " 76,\n",
       " 77,\n",
       " 78,\n",
       " 79,\n",
       " 80,\n",
       " 81,\n",
       " 82,\n",
       " 83,\n",
       " 84,\n",
       " 85,\n",
       " 86,\n",
       " 87,\n",
       " 88,\n",
       " 89,\n",
       " 90,\n",
       " 91,\n",
       " 92,\n",
       " 93,\n",
       " 94,\n",
       " 95,\n",
       " 96,\n",
       " 97,\n",
       " 98,\n",
       " 99]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 0: 339 samples\n",
      "Class 1: 131 samples\n",
      "Class 2: 68 samples\n",
      "Class 3: 117 samples\n",
      "Class 4: 70 samples\n",
      "Class 5: 68 samples\n",
      "Class 6: 80 samples\n",
      "Class 7: 122 samples\n",
      "Class 8: 31 samples\n",
      "Class 9: 86 samples\n",
      "Class 10: 84 samples\n",
      "Class 11: 63 samples\n",
      "Class 12: 326 samples\n",
      "Class 13: 101 samples\n",
      "Class 14: 79 samples\n",
      "Class 15: 106 samples\n",
      "Class 16: 128 samples\n",
      "Class 17: 75 samples\n",
      "Class 18: 91 samples\n",
      "Class 19: 272 samples\n",
      "Class 20: 55 samples\n",
      "Class 21: 56 samples\n",
      "Class 22: 60 samples\n",
      "Class 23: 60 samples\n",
      "Class 24: 54 samples\n",
      "Class 25: 69 samples\n",
      "Class 26: 116 samples\n",
      "Class 27: 48 samples\n",
      "Class 28: 98 samples\n",
      "Class 29: 106 samples\n",
      "Class 30: 78 samples\n",
      "Class 31: 81 samples\n",
      "Class 32: 46 samples\n",
      "Class 33: 81 samples\n",
      "Class 34: 79 samples\n",
      "Class 35: 89 samples\n",
      "Class 36: 81 samples\n",
      "Class 37: 86 samples\n",
      "Class 38: 60 samples\n",
      "Class 39: 332 samples\n",
      "Class 40: 62 samples\n",
      "Class 41: 76 samples\n",
      "Class 42: 112 samples\n",
      "Class 43: 65 samples\n",
      "Class 44: 67 samples\n",
      "Class 45: 62 samples\n",
      "Class 46: 49 samples\n",
      "Class 47: 266 samples\n",
      "Class 48: 52 samples\n",
      "Class 49: 79 samples\n",
      "Class 50: 85 samples\n",
      "Class 51: 90 samples\n",
      "Class 52: 51 samples\n",
      "Class 53: 54 samples\n",
      "Class 54: 76 samples\n",
      "Class 55: 73 samples\n",
      "Class 56: 76 samples\n",
      "Class 57: 113 samples\n",
      "Class 58: 104 samples\n",
      "Class 59: 146 samples\n",
      "Class 60: 64 samples\n",
      "Class 61: 116 samples\n",
      "Class 62: 75 samples\n",
      "Class 63: 37 samples\n",
      "Class 64: 60 samples\n",
      "Class 65: 57 samples\n",
      "Class 66: 57 samples\n",
      "Class 67: 52 samples\n",
      "Class 68: 61 samples\n",
      "Class 69: 65 samples\n",
      "Class 70: 80 samples\n",
      "Class 71: 64 samples\n",
      "Class 72: 57 samples\n",
      "Class 73: 38 samples\n",
      "Class 74: 78 samples\n",
      "Class 75: 34 samples\n",
      "Class 76: 121 samples\n",
      "Class 77: 56 samples\n",
      "Class 78: 53 samples\n",
      "Class 79: 88 samples\n",
      "Class 80: 56 samples\n",
      "Class 81: 59 samples\n",
      "Class 82: 60 samples\n",
      "Class 83: 44 samples\n",
      "Class 84: 74 samples\n",
      "Class 85: 60 samples\n",
      "Class 86: 67 samples\n",
      "Class 87: 146 samples\n",
      "Class 88: 64 samples\n",
      "Class 89: 50 samples\n",
      "Class 90: 43 samples\n",
      "Class 91: 34 samples\n",
      "Class 92: 46 samples\n",
      "Class 93: 60 samples\n",
      "Class 94: 51 samples\n",
      "Class 95: 57 samples\n",
      "Class 96: 63 samples\n",
      "Class 97: 51 samples\n",
      "Class 98: 101 samples\n",
      "Class 99: 48 samples\n"
     ]
    }
   ],
   "source": [
    "unique_classes = np.sort(np.unique(y_test))\n",
    "# Create new ordered arrays\n",
    "z_test_ordered = []\n",
    "y_test_ordered = []\n",
    "\n",
    "# For each class, collect all samples\n",
    "for class_label in unique_classes:\n",
    "    # Find all indices where y_test == class_label\n",
    "    class_indices = np.where(y_test == class_label)[0]\n",
    "    \n",
    "    # Extract embeddings and labels for this class\n",
    "    z_test_ordered.append(z_test[class_indices])\n",
    "    y_test_ordered.append(y_test[class_indices])\n",
    "    \n",
    "    print(f\"Class {class_label}: {len(class_indices)} samples\")\n",
    "\n",
    "# Concatenate all classes\n",
    "z_test_ordered = np.vstack(z_test_ordered)\n",
    "y_test_ordered = np.concatenate(y_test_ordered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  0,  0, ..., 99, 99, 99], shape=(8477,))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_ordered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\teval-mlogloss:1.37009\ttrain-mlogloss:1.31523\n",
      "[1]\teval-mlogloss:1.23441\ttrain-mlogloss:1.17068\n",
      "[2]\teval-mlogloss:1.13312\ttrain-mlogloss:1.05966\n",
      "[3]\teval-mlogloss:1.04891\ttrain-mlogloss:0.96745\n",
      "[4]\teval-mlogloss:0.97775\ttrain-mlogloss:0.88860\n",
      "[5]\teval-mlogloss:0.91631\ttrain-mlogloss:0.81965\n",
      "[6]\teval-mlogloss:0.86230\ttrain-mlogloss:0.75831\n",
      "[7]\teval-mlogloss:0.81462\ttrain-mlogloss:0.70415\n",
      "[8]\teval-mlogloss:0.77197\ttrain-mlogloss:0.65508\n",
      "[9]\teval-mlogloss:0.73356\ttrain-mlogloss:0.61064\n",
      "[10]\teval-mlogloss:0.69919\ttrain-mlogloss:0.57050\n",
      "[11]\teval-mlogloss:0.66854\ttrain-mlogloss:0.53377\n",
      "[12]\teval-mlogloss:0.64089\ttrain-mlogloss:0.50028\n",
      "[13]\teval-mlogloss:0.61592\ttrain-mlogloss:0.46938\n",
      "[14]\teval-mlogloss:0.59297\ttrain-mlogloss:0.44118\n",
      "[15]\teval-mlogloss:0.57225\ttrain-mlogloss:0.41507\n",
      "[16]\teval-mlogloss:0.55303\ttrain-mlogloss:0.39088\n",
      "[17]\teval-mlogloss:0.53531\ttrain-mlogloss:0.36870\n",
      "[18]\teval-mlogloss:0.51933\ttrain-mlogloss:0.34807\n",
      "[19]\teval-mlogloss:0.50475\ttrain-mlogloss:0.32901\n",
      "[20]\teval-mlogloss:0.49131\ttrain-mlogloss:0.31146\n",
      "[21]\teval-mlogloss:0.47900\ttrain-mlogloss:0.29495\n",
      "[22]\teval-mlogloss:0.46786\ttrain-mlogloss:0.27967\n",
      "[23]\teval-mlogloss:0.45768\ttrain-mlogloss:0.26553\n",
      "[24]\teval-mlogloss:0.44822\ttrain-mlogloss:0.25226\n",
      "[25]\teval-mlogloss:0.43954\ttrain-mlogloss:0.23994\n",
      "[26]\teval-mlogloss:0.43151\ttrain-mlogloss:0.22832\n",
      "[27]\teval-mlogloss:0.42419\ttrain-mlogloss:0.21744\n",
      "[28]\teval-mlogloss:0.41737\ttrain-mlogloss:0.20717\n",
      "[29]\teval-mlogloss:0.41123\ttrain-mlogloss:0.19759\n",
      "[30]\teval-mlogloss:0.40550\ttrain-mlogloss:0.18857\n",
      "[31]\teval-mlogloss:0.39989\ttrain-mlogloss:0.18000\n",
      "[32]\teval-mlogloss:0.39504\ttrain-mlogloss:0.17210\n",
      "[33]\teval-mlogloss:0.39053\ttrain-mlogloss:0.16447\n",
      "[34]\teval-mlogloss:0.38647\ttrain-mlogloss:0.15733\n",
      "[35]\teval-mlogloss:0.38257\ttrain-mlogloss:0.15071\n",
      "[36]\teval-mlogloss:0.37903\ttrain-mlogloss:0.14458\n",
      "[37]\teval-mlogloss:0.37564\ttrain-mlogloss:0.13867\n",
      "[38]\teval-mlogloss:0.37238\ttrain-mlogloss:0.13312\n",
      "[39]\teval-mlogloss:0.36943\ttrain-mlogloss:0.12760\n",
      "[40]\teval-mlogloss:0.36673\ttrain-mlogloss:0.12266\n",
      "[41]\teval-mlogloss:0.36414\ttrain-mlogloss:0.11756\n",
      "[42]\teval-mlogloss:0.36187\ttrain-mlogloss:0.11299\n",
      "[43]\teval-mlogloss:0.35955\ttrain-mlogloss:0.10843\n",
      "[44]\teval-mlogloss:0.35755\ttrain-mlogloss:0.10443\n",
      "[45]\teval-mlogloss:0.35573\ttrain-mlogloss:0.10069\n",
      "[46]\teval-mlogloss:0.35397\ttrain-mlogloss:0.09708\n",
      "[47]\teval-mlogloss:0.35243\ttrain-mlogloss:0.09370\n",
      "[48]\teval-mlogloss:0.35084\ttrain-mlogloss:0.09053\n",
      "[49]\teval-mlogloss:0.34951\ttrain-mlogloss:0.08748\n",
      "[50]\teval-mlogloss:0.34823\ttrain-mlogloss:0.08451\n",
      "[51]\teval-mlogloss:0.34696\ttrain-mlogloss:0.08174\n",
      "[52]\teval-mlogloss:0.34579\ttrain-mlogloss:0.07921\n",
      "[53]\teval-mlogloss:0.34472\ttrain-mlogloss:0.07687\n",
      "[54]\teval-mlogloss:0.34384\ttrain-mlogloss:0.07463\n",
      "[55]\teval-mlogloss:0.34312\ttrain-mlogloss:0.07235\n",
      "[56]\teval-mlogloss:0.34250\ttrain-mlogloss:0.07029\n",
      "[57]\teval-mlogloss:0.34195\ttrain-mlogloss:0.06818\n",
      "[58]\teval-mlogloss:0.34121\ttrain-mlogloss:0.06622\n",
      "[59]\teval-mlogloss:0.34053\ttrain-mlogloss:0.06449\n",
      "[60]\teval-mlogloss:0.33989\ttrain-mlogloss:0.06267\n",
      "[61]\teval-mlogloss:0.33951\ttrain-mlogloss:0.06103\n",
      "[62]\teval-mlogloss:0.33901\ttrain-mlogloss:0.05947\n",
      "[63]\teval-mlogloss:0.33858\ttrain-mlogloss:0.05792\n",
      "[64]\teval-mlogloss:0.33811\ttrain-mlogloss:0.05640\n",
      "[65]\teval-mlogloss:0.33785\ttrain-mlogloss:0.05501\n",
      "[66]\teval-mlogloss:0.33756\ttrain-mlogloss:0.05367\n",
      "[67]\teval-mlogloss:0.33741\ttrain-mlogloss:0.05236\n",
      "[68]\teval-mlogloss:0.33738\ttrain-mlogloss:0.05111\n",
      "[69]\teval-mlogloss:0.33710\ttrain-mlogloss:0.04989\n",
      "[70]\teval-mlogloss:0.33674\ttrain-mlogloss:0.04878\n",
      "[71]\teval-mlogloss:0.33660\ttrain-mlogloss:0.04761\n",
      "[72]\teval-mlogloss:0.33651\ttrain-mlogloss:0.04642\n",
      "[73]\teval-mlogloss:0.33653\ttrain-mlogloss:0.04549\n",
      "[74]\teval-mlogloss:0.33648\ttrain-mlogloss:0.04452\n",
      "[75]\teval-mlogloss:0.33624\ttrain-mlogloss:0.04351\n",
      "[76]\teval-mlogloss:0.33620\ttrain-mlogloss:0.04273\n",
      "[77]\teval-mlogloss:0.33616\ttrain-mlogloss:0.04179\n",
      "[78]\teval-mlogloss:0.33630\ttrain-mlogloss:0.04092\n",
      "[79]\teval-mlogloss:0.33620\ttrain-mlogloss:0.04007\n",
      "[80]\teval-mlogloss:0.33632\ttrain-mlogloss:0.03918\n",
      "[81]\teval-mlogloss:0.33636\ttrain-mlogloss:0.03845\n",
      "[82]\teval-mlogloss:0.33632\ttrain-mlogloss:0.03777\n",
      "[83]\teval-mlogloss:0.33633\ttrain-mlogloss:0.03712\n",
      "[84]\teval-mlogloss:0.33643\ttrain-mlogloss:0.03649\n",
      "[85]\teval-mlogloss:0.33641\ttrain-mlogloss:0.03579\n",
      "[86]\teval-mlogloss:0.33650\ttrain-mlogloss:0.03518\n",
      "[87]\teval-mlogloss:0.33654\ttrain-mlogloss:0.03457\n",
      "[88]\teval-mlogloss:0.33661\ttrain-mlogloss:0.03389\n",
      "[89]\teval-mlogloss:0.33668\ttrain-mlogloss:0.03331\n",
      "[90]\teval-mlogloss:0.33670\ttrain-mlogloss:0.03272\n",
      "[91]\teval-mlogloss:0.33672\ttrain-mlogloss:0.03211\n",
      "[92]\teval-mlogloss:0.33690\ttrain-mlogloss:0.03150\n",
      "[93]\teval-mlogloss:0.33717\ttrain-mlogloss:0.03090\n",
      "[94]\teval-mlogloss:0.33721\ttrain-mlogloss:0.03044\n",
      "[95]\teval-mlogloss:0.33725\ttrain-mlogloss:0.02999\n",
      "[96]\teval-mlogloss:0.33738\ttrain-mlogloss:0.02942\n",
      "[97]\teval-mlogloss:0.33730\ttrain-mlogloss:0.02902\n",
      "[98]\teval-mlogloss:0.33748\ttrain-mlogloss:0.02859\n",
      "[99]\teval-mlogloss:0.33753\ttrain-mlogloss:0.02816\n",
      "[100]\teval-mlogloss:0.33763\ttrain-mlogloss:0.02778\n",
      "[101]\teval-mlogloss:0.33784\ttrain-mlogloss:0.02730\n",
      "[102]\teval-mlogloss:0.33803\ttrain-mlogloss:0.02689\n",
      "[103]\teval-mlogloss:0.33821\ttrain-mlogloss:0.02646\n",
      "[104]\teval-mlogloss:0.33835\ttrain-mlogloss:0.02604\n",
      "[105]\teval-mlogloss:0.33850\ttrain-mlogloss:0.02573\n",
      "[106]\teval-mlogloss:0.33862\ttrain-mlogloss:0.02537\n",
      "[107]\teval-mlogloss:0.33876\ttrain-mlogloss:0.02505\n",
      "[108]\teval-mlogloss:0.33878\ttrain-mlogloss:0.02462\n",
      "[109]\teval-mlogloss:0.33896\ttrain-mlogloss:0.02425\n",
      "[110]\teval-mlogloss:0.33922\ttrain-mlogloss:0.02387\n",
      "[111]\teval-mlogloss:0.33930\ttrain-mlogloss:0.02351\n",
      "[112]\teval-mlogloss:0.33944\ttrain-mlogloss:0.02320\n",
      "[113]\teval-mlogloss:0.33952\ttrain-mlogloss:0.02290\n",
      "[114]\teval-mlogloss:0.33961\ttrain-mlogloss:0.02259\n",
      "[115]\teval-mlogloss:0.33975\ttrain-mlogloss:0.02230\n",
      "[116]\teval-mlogloss:0.33989\ttrain-mlogloss:0.02204\n",
      "[117]\teval-mlogloss:0.34021\ttrain-mlogloss:0.02172\n",
      "[118]\teval-mlogloss:0.34037\ttrain-mlogloss:0.02151\n",
      "[119]\teval-mlogloss:0.34054\ttrain-mlogloss:0.02124\n",
      "[120]\teval-mlogloss:0.34062\ttrain-mlogloss:0.02094\n",
      "[121]\teval-mlogloss:0.34077\ttrain-mlogloss:0.02066\n",
      "[122]\teval-mlogloss:0.34088\ttrain-mlogloss:0.02036\n",
      "[123]\teval-mlogloss:0.34105\ttrain-mlogloss:0.02013\n",
      "[124]\teval-mlogloss:0.34119\ttrain-mlogloss:0.01992\n",
      "[125]\teval-mlogloss:0.34135\ttrain-mlogloss:0.01966\n",
      "[126]\teval-mlogloss:0.34144\ttrain-mlogloss:0.01943\n",
      "[127]\teval-mlogloss:0.34160\ttrain-mlogloss:0.01921\n",
      "[128]\teval-mlogloss:0.34170\ttrain-mlogloss:0.01898\n",
      "[129]\teval-mlogloss:0.34181\ttrain-mlogloss:0.01867\n",
      "[130]\teval-mlogloss:0.34184\ttrain-mlogloss:0.01844\n",
      "[131]\teval-mlogloss:0.34185\ttrain-mlogloss:0.01821\n",
      "[132]\teval-mlogloss:0.34192\ttrain-mlogloss:0.01802\n",
      "[133]\teval-mlogloss:0.34200\ttrain-mlogloss:0.01782\n",
      "[134]\teval-mlogloss:0.34215\ttrain-mlogloss:0.01764\n",
      "[135]\teval-mlogloss:0.34237\ttrain-mlogloss:0.01738\n",
      "[136]\teval-mlogloss:0.34256\ttrain-mlogloss:0.01719\n",
      "[137]\teval-mlogloss:0.34264\ttrain-mlogloss:0.01702\n",
      "[138]\teval-mlogloss:0.34281\ttrain-mlogloss:0.01678\n",
      "[139]\teval-mlogloss:0.34291\ttrain-mlogloss:0.01659\n",
      "[140]\teval-mlogloss:0.34310\ttrain-mlogloss:0.01638\n",
      "[141]\teval-mlogloss:0.34319\ttrain-mlogloss:0.01621\n",
      "[142]\teval-mlogloss:0.34329\ttrain-mlogloss:0.01601\n",
      "[143]\teval-mlogloss:0.34336\ttrain-mlogloss:0.01576\n",
      "[144]\teval-mlogloss:0.34353\ttrain-mlogloss:0.01556\n",
      "[145]\teval-mlogloss:0.34370\ttrain-mlogloss:0.01535\n",
      "[146]\teval-mlogloss:0.34381\ttrain-mlogloss:0.01519\n",
      "[147]\teval-mlogloss:0.34387\ttrain-mlogloss:0.01501\n",
      "[148]\teval-mlogloss:0.34395\ttrain-mlogloss:0.01481\n",
      "[149]\teval-mlogloss:0.34412\ttrain-mlogloss:0.01468\n",
      "[150]\teval-mlogloss:0.34422\ttrain-mlogloss:0.01454\n",
      "[151]\teval-mlogloss:0.34431\ttrain-mlogloss:0.01432\n",
      "[152]\teval-mlogloss:0.34437\ttrain-mlogloss:0.01418\n",
      "[153]\teval-mlogloss:0.34445\ttrain-mlogloss:0.01403\n",
      "[154]\teval-mlogloss:0.34466\ttrain-mlogloss:0.01386\n",
      "[155]\teval-mlogloss:0.34478\ttrain-mlogloss:0.01368\n",
      "[156]\teval-mlogloss:0.34487\ttrain-mlogloss:0.01353\n",
      "[157]\teval-mlogloss:0.34496\ttrain-mlogloss:0.01336\n",
      "[158]\teval-mlogloss:0.34500\ttrain-mlogloss:0.01322\n",
      "[159]\teval-mlogloss:0.34511\ttrain-mlogloss:0.01309\n",
      "[160]\teval-mlogloss:0.34523\ttrain-mlogloss:0.01295\n",
      "[161]\teval-mlogloss:0.34539\ttrain-mlogloss:0.01283\n",
      "[162]\teval-mlogloss:0.34540\ttrain-mlogloss:0.01270\n",
      "[163]\teval-mlogloss:0.34547\ttrain-mlogloss:0.01257\n",
      "[164]\teval-mlogloss:0.34566\ttrain-mlogloss:0.01245\n",
      "[165]\teval-mlogloss:0.34570\ttrain-mlogloss:0.01229\n",
      "[166]\teval-mlogloss:0.34577\ttrain-mlogloss:0.01215\n",
      "[167]\teval-mlogloss:0.34589\ttrain-mlogloss:0.01200\n",
      "[168]\teval-mlogloss:0.34607\ttrain-mlogloss:0.01190\n",
      "[169]\teval-mlogloss:0.34610\ttrain-mlogloss:0.01176\n",
      "[170]\teval-mlogloss:0.34618\ttrain-mlogloss:0.01165\n",
      "[171]\teval-mlogloss:0.34626\ttrain-mlogloss:0.01151\n",
      "[172]\teval-mlogloss:0.34631\ttrain-mlogloss:0.01137\n",
      "[173]\teval-mlogloss:0.34643\ttrain-mlogloss:0.01124\n",
      "[174]\teval-mlogloss:0.34657\ttrain-mlogloss:0.01115\n",
      "[175]\teval-mlogloss:0.34666\ttrain-mlogloss:0.01103\n",
      "[176]\teval-mlogloss:0.34684\ttrain-mlogloss:0.01093\n",
      "[177]\teval-mlogloss:0.34700\ttrain-mlogloss:0.01081\n",
      "[178]\teval-mlogloss:0.34708\ttrain-mlogloss:0.01069\n",
      "[179]\teval-mlogloss:0.34717\ttrain-mlogloss:0.01057\n",
      "[180]\teval-mlogloss:0.34725\ttrain-mlogloss:0.01047\n",
      "[181]\teval-mlogloss:0.34736\ttrain-mlogloss:0.01035\n",
      "[182]\teval-mlogloss:0.34746\ttrain-mlogloss:0.01027\n",
      "[183]\teval-mlogloss:0.34758\ttrain-mlogloss:0.01018\n",
      "[184]\teval-mlogloss:0.34771\ttrain-mlogloss:0.01008\n",
      "[185]\teval-mlogloss:0.34784\ttrain-mlogloss:0.00999\n",
      "[186]\teval-mlogloss:0.34794\ttrain-mlogloss:0.00990\n",
      "[187]\teval-mlogloss:0.34797\ttrain-mlogloss:0.00980\n",
      "[188]\teval-mlogloss:0.34807\ttrain-mlogloss:0.00971\n",
      "[189]\teval-mlogloss:0.34814\ttrain-mlogloss:0.00963\n",
      "[190]\teval-mlogloss:0.34838\ttrain-mlogloss:0.00953\n",
      "[191]\teval-mlogloss:0.34849\ttrain-mlogloss:0.00944\n",
      "[192]\teval-mlogloss:0.34856\ttrain-mlogloss:0.00935\n",
      "[193]\teval-mlogloss:0.34864\ttrain-mlogloss:0.00925\n",
      "[194]\teval-mlogloss:0.34877\ttrain-mlogloss:0.00919\n",
      "[195]\teval-mlogloss:0.34883\ttrain-mlogloss:0.00913\n",
      "[196]\teval-mlogloss:0.34893\ttrain-mlogloss:0.00905\n",
      "[197]\teval-mlogloss:0.34901\ttrain-mlogloss:0.00895\n",
      "[198]\teval-mlogloss:0.34919\ttrain-mlogloss:0.00889\n",
      "[199]\teval-mlogloss:0.34929\ttrain-mlogloss:0.00881\n",
      "[200]\teval-mlogloss:0.34935\ttrain-mlogloss:0.00872\n",
      "[201]\teval-mlogloss:0.34947\ttrain-mlogloss:0.00863\n",
      "[202]\teval-mlogloss:0.34949\ttrain-mlogloss:0.00854\n",
      "[203]\teval-mlogloss:0.34967\ttrain-mlogloss:0.00844\n",
      "[204]\teval-mlogloss:0.34974\ttrain-mlogloss:0.00837\n",
      "[205]\teval-mlogloss:0.34988\ttrain-mlogloss:0.00830\n",
      "[206]\teval-mlogloss:0.34992\ttrain-mlogloss:0.00825\n",
      "[207]\teval-mlogloss:0.34991\ttrain-mlogloss:0.00816\n",
      "[208]\teval-mlogloss:0.34994\ttrain-mlogloss:0.00807\n",
      "[209]\teval-mlogloss:0.34997\ttrain-mlogloss:0.00798\n",
      "[210]\teval-mlogloss:0.35010\ttrain-mlogloss:0.00791\n",
      "[211]\teval-mlogloss:0.35011\ttrain-mlogloss:0.00783\n",
      "[212]\teval-mlogloss:0.35026\ttrain-mlogloss:0.00775\n",
      "[213]\teval-mlogloss:0.35027\ttrain-mlogloss:0.00767\n",
      "[214]\teval-mlogloss:0.35035\ttrain-mlogloss:0.00760\n",
      "[215]\teval-mlogloss:0.35040\ttrain-mlogloss:0.00754\n",
      "[216]\teval-mlogloss:0.35051\ttrain-mlogloss:0.00747\n",
      "[217]\teval-mlogloss:0.35053\ttrain-mlogloss:0.00741\n",
      "[218]\teval-mlogloss:0.35060\ttrain-mlogloss:0.00736\n",
      "[219]\teval-mlogloss:0.35069\ttrain-mlogloss:0.00728\n",
      "[220]\teval-mlogloss:0.35073\ttrain-mlogloss:0.00722\n",
      "[221]\teval-mlogloss:0.35067\ttrain-mlogloss:0.00714\n",
      "[222]\teval-mlogloss:0.35063\ttrain-mlogloss:0.00708\n",
      "[223]\teval-mlogloss:0.35072\ttrain-mlogloss:0.00703\n",
      "[224]\teval-mlogloss:0.35075\ttrain-mlogloss:0.00697\n",
      "[225]\teval-mlogloss:0.35081\ttrain-mlogloss:0.00691\n",
      "[226]\teval-mlogloss:0.35088\ttrain-mlogloss:0.00684\n",
      "[227]\teval-mlogloss:0.35097\ttrain-mlogloss:0.00679\n",
      "[228]\teval-mlogloss:0.35106\ttrain-mlogloss:0.00672\n",
      "[229]\teval-mlogloss:0.35113\ttrain-mlogloss:0.00666\n",
      "[230]\teval-mlogloss:0.35123\ttrain-mlogloss:0.00661\n",
      "[231]\teval-mlogloss:0.35129\ttrain-mlogloss:0.00656\n",
      "[232]\teval-mlogloss:0.35136\ttrain-mlogloss:0.00650\n",
      "[233]\teval-mlogloss:0.35133\ttrain-mlogloss:0.00645\n",
      "[234]\teval-mlogloss:0.35136\ttrain-mlogloss:0.00640\n",
      "[235]\teval-mlogloss:0.35147\ttrain-mlogloss:0.00636\n",
      "[236]\teval-mlogloss:0.35152\ttrain-mlogloss:0.00630\n",
      "[237]\teval-mlogloss:0.35159\ttrain-mlogloss:0.00624\n",
      "[238]\teval-mlogloss:0.35158\ttrain-mlogloss:0.00619\n",
      "[239]\teval-mlogloss:0.35164\ttrain-mlogloss:0.00614\n",
      "[240]\teval-mlogloss:0.35168\ttrain-mlogloss:0.00608\n",
      "[241]\teval-mlogloss:0.35177\ttrain-mlogloss:0.00603\n",
      "[242]\teval-mlogloss:0.35184\ttrain-mlogloss:0.00600\n",
      "[243]\teval-mlogloss:0.35193\ttrain-mlogloss:0.00593\n",
      "[244]\teval-mlogloss:0.35201\ttrain-mlogloss:0.00589\n",
      "[245]\teval-mlogloss:0.35203\ttrain-mlogloss:0.00584\n",
      "[246]\teval-mlogloss:0.35206\ttrain-mlogloss:0.00578\n",
      "[247]\teval-mlogloss:0.35209\ttrain-mlogloss:0.00573\n",
      "[248]\teval-mlogloss:0.35211\ttrain-mlogloss:0.00568\n",
      "[249]\teval-mlogloss:0.35218\ttrain-mlogloss:0.00563\n",
      "[250]\teval-mlogloss:0.35222\ttrain-mlogloss:0.00558\n",
      "[251]\teval-mlogloss:0.35223\ttrain-mlogloss:0.00555\n",
      "[252]\teval-mlogloss:0.35227\ttrain-mlogloss:0.00552\n",
      "[253]\teval-mlogloss:0.35235\ttrain-mlogloss:0.00548\n",
      "[254]\teval-mlogloss:0.35244\ttrain-mlogloss:0.00545\n",
      "[255]\teval-mlogloss:0.35245\ttrain-mlogloss:0.00541\n",
      "[256]\teval-mlogloss:0.35254\ttrain-mlogloss:0.00537\n",
      "[257]\teval-mlogloss:0.35263\ttrain-mlogloss:0.00534\n",
      "[258]\teval-mlogloss:0.35270\ttrain-mlogloss:0.00530\n",
      "[259]\teval-mlogloss:0.35278\ttrain-mlogloss:0.00526\n",
      "[260]\teval-mlogloss:0.35290\ttrain-mlogloss:0.00523\n",
      "[261]\teval-mlogloss:0.35293\ttrain-mlogloss:0.00519\n",
      "[262]\teval-mlogloss:0.35297\ttrain-mlogloss:0.00515\n",
      "[263]\teval-mlogloss:0.35295\ttrain-mlogloss:0.00511\n",
      "[264]\teval-mlogloss:0.35303\ttrain-mlogloss:0.00508\n",
      "[265]\teval-mlogloss:0.35311\ttrain-mlogloss:0.00505\n",
      "[266]\teval-mlogloss:0.35315\ttrain-mlogloss:0.00502\n",
      "[267]\teval-mlogloss:0.35324\ttrain-mlogloss:0.00498\n",
      "[268]\teval-mlogloss:0.35330\ttrain-mlogloss:0.00494\n",
      "[269]\teval-mlogloss:0.35339\ttrain-mlogloss:0.00490\n",
      "[270]\teval-mlogloss:0.35349\ttrain-mlogloss:0.00487\n",
      "[271]\teval-mlogloss:0.35350\ttrain-mlogloss:0.00483\n",
      "[272]\teval-mlogloss:0.35355\ttrain-mlogloss:0.00479\n",
      "[273]\teval-mlogloss:0.35367\ttrain-mlogloss:0.00476\n",
      "[274]\teval-mlogloss:0.35373\ttrain-mlogloss:0.00473\n",
      "[275]\teval-mlogloss:0.35387\ttrain-mlogloss:0.00470\n",
      "[276]\teval-mlogloss:0.35391\ttrain-mlogloss:0.00468\n",
      "[277]\teval-mlogloss:0.35396\ttrain-mlogloss:0.00465\n",
      "[278]\teval-mlogloss:0.35404\ttrain-mlogloss:0.00461\n",
      "[279]\teval-mlogloss:0.35414\ttrain-mlogloss:0.00458\n",
      "[280]\teval-mlogloss:0.35412\ttrain-mlogloss:0.00455\n",
      "[281]\teval-mlogloss:0.35419\ttrain-mlogloss:0.00452\n",
      "[282]\teval-mlogloss:0.35420\ttrain-mlogloss:0.00449\n",
      "[283]\teval-mlogloss:0.35415\ttrain-mlogloss:0.00444\n",
      "[284]\teval-mlogloss:0.35428\ttrain-mlogloss:0.00441\n",
      "[285]\teval-mlogloss:0.35425\ttrain-mlogloss:0.00438\n",
      "[286]\teval-mlogloss:0.35429\ttrain-mlogloss:0.00434\n",
      "[287]\teval-mlogloss:0.35437\ttrain-mlogloss:0.00431\n",
      "[288]\teval-mlogloss:0.35444\ttrain-mlogloss:0.00428\n",
      "[289]\teval-mlogloss:0.35442\ttrain-mlogloss:0.00425\n",
      "[290]\teval-mlogloss:0.35446\ttrain-mlogloss:0.00422\n",
      "[291]\teval-mlogloss:0.35459\ttrain-mlogloss:0.00420\n",
      "[292]\teval-mlogloss:0.35464\ttrain-mlogloss:0.00417\n",
      "[293]\teval-mlogloss:0.35479\ttrain-mlogloss:0.00414\n",
      "[294]\teval-mlogloss:0.35487\ttrain-mlogloss:0.00412\n",
      "[295]\teval-mlogloss:0.35492\ttrain-mlogloss:0.00409\n",
      "[296]\teval-mlogloss:0.35503\ttrain-mlogloss:0.00407\n",
      "[297]\teval-mlogloss:0.35505\ttrain-mlogloss:0.00404\n",
      "[298]\teval-mlogloss:0.35509\ttrain-mlogloss:0.00402\n",
      "[299]\teval-mlogloss:0.35508\ttrain-mlogloss:0.00400\n",
      "[300]\teval-mlogloss:0.35520\ttrain-mlogloss:0.00397\n",
      "[301]\teval-mlogloss:0.35526\ttrain-mlogloss:0.00394\n",
      "[302]\teval-mlogloss:0.35530\ttrain-mlogloss:0.00392\n",
      "[303]\teval-mlogloss:0.35542\ttrain-mlogloss:0.00389\n",
      "[304]\teval-mlogloss:0.35548\ttrain-mlogloss:0.00387\n",
      "[305]\teval-mlogloss:0.35555\ttrain-mlogloss:0.00385\n",
      "[306]\teval-mlogloss:0.35557\ttrain-mlogloss:0.00383\n",
      "[307]\teval-mlogloss:0.35566\ttrain-mlogloss:0.00381\n",
      "[308]\teval-mlogloss:0.35573\ttrain-mlogloss:0.00379\n",
      "[309]\teval-mlogloss:0.35578\ttrain-mlogloss:0.00377\n",
      "[310]\teval-mlogloss:0.35579\ttrain-mlogloss:0.00375\n",
      "[311]\teval-mlogloss:0.35587\ttrain-mlogloss:0.00372\n",
      "[312]\teval-mlogloss:0.35583\ttrain-mlogloss:0.00370\n",
      "[313]\teval-mlogloss:0.35584\ttrain-mlogloss:0.00367\n",
      "[314]\teval-mlogloss:0.35594\ttrain-mlogloss:0.00365\n",
      "[315]\teval-mlogloss:0.35597\ttrain-mlogloss:0.00362\n",
      "[316]\teval-mlogloss:0.35599\ttrain-mlogloss:0.00360\n",
      "[317]\teval-mlogloss:0.35607\ttrain-mlogloss:0.00358\n",
      "[318]\teval-mlogloss:0.35612\ttrain-mlogloss:0.00356\n",
      "[319]\teval-mlogloss:0.35616\ttrain-mlogloss:0.00354\n",
      "[320]\teval-mlogloss:0.35612\ttrain-mlogloss:0.00351\n",
      "[321]\teval-mlogloss:0.35615\ttrain-mlogloss:0.00349\n",
      "[322]\teval-mlogloss:0.35618\ttrain-mlogloss:0.00347\n",
      "[323]\teval-mlogloss:0.35623\ttrain-mlogloss:0.00345\n",
      "[324]\teval-mlogloss:0.35629\ttrain-mlogloss:0.00344\n",
      "[325]\teval-mlogloss:0.35635\ttrain-mlogloss:0.00341\n",
      "[326]\teval-mlogloss:0.35636\ttrain-mlogloss:0.00339\n",
      "[327]\teval-mlogloss:0.35640\ttrain-mlogloss:0.00337\n",
      "[328]\teval-mlogloss:0.35643\ttrain-mlogloss:0.00335\n",
      "[329]\teval-mlogloss:0.35642\ttrain-mlogloss:0.00333\n",
      "[330]\teval-mlogloss:0.35647\ttrain-mlogloss:0.00331\n",
      "[331]\teval-mlogloss:0.35656\ttrain-mlogloss:0.00329\n",
      "[332]\teval-mlogloss:0.35660\ttrain-mlogloss:0.00327\n",
      "[333]\teval-mlogloss:0.35659\ttrain-mlogloss:0.00324\n",
      "[334]\teval-mlogloss:0.35663\ttrain-mlogloss:0.00322\n",
      "[335]\teval-mlogloss:0.35671\ttrain-mlogloss:0.00320\n",
      "[336]\teval-mlogloss:0.35679\ttrain-mlogloss:0.00319\n",
      "[337]\teval-mlogloss:0.35684\ttrain-mlogloss:0.00317\n",
      "[338]\teval-mlogloss:0.35684\ttrain-mlogloss:0.00315\n",
      "[339]\teval-mlogloss:0.35683\ttrain-mlogloss:0.00313\n",
      "[340]\teval-mlogloss:0.35690\ttrain-mlogloss:0.00311\n",
      "[341]\teval-mlogloss:0.35695\ttrain-mlogloss:0.00309\n",
      "[342]\teval-mlogloss:0.35696\ttrain-mlogloss:0.00308\n",
      "[343]\teval-mlogloss:0.35701\ttrain-mlogloss:0.00306\n",
      "[344]\teval-mlogloss:0.35711\ttrain-mlogloss:0.00304\n",
      "[345]\teval-mlogloss:0.35721\ttrain-mlogloss:0.00303\n",
      "[346]\teval-mlogloss:0.35731\ttrain-mlogloss:0.00301\n",
      "[347]\teval-mlogloss:0.35739\ttrain-mlogloss:0.00300\n",
      "[348]\teval-mlogloss:0.35744\ttrain-mlogloss:0.00298\n",
      "[349]\teval-mlogloss:0.35748\ttrain-mlogloss:0.00298\n",
      "[350]\teval-mlogloss:0.35752\ttrain-mlogloss:0.00296\n",
      "[351]\teval-mlogloss:0.35754\ttrain-mlogloss:0.00294\n",
      "[352]\teval-mlogloss:0.35757\ttrain-mlogloss:0.00293\n",
      "[353]\teval-mlogloss:0.35765\ttrain-mlogloss:0.00291\n",
      "[354]\teval-mlogloss:0.35768\ttrain-mlogloss:0.00289\n",
      "[355]\teval-mlogloss:0.35771\ttrain-mlogloss:0.00289\n",
      "[356]\teval-mlogloss:0.35775\ttrain-mlogloss:0.00287\n",
      "[357]\teval-mlogloss:0.35787\ttrain-mlogloss:0.00286\n",
      "[358]\teval-mlogloss:0.35790\ttrain-mlogloss:0.00285\n",
      "[359]\teval-mlogloss:0.35798\ttrain-mlogloss:0.00284\n",
      "[360]\teval-mlogloss:0.35802\ttrain-mlogloss:0.00281\n",
      "[361]\teval-mlogloss:0.35806\ttrain-mlogloss:0.00280\n",
      "[362]\teval-mlogloss:0.35810\ttrain-mlogloss:0.00279\n",
      "[363]\teval-mlogloss:0.35816\ttrain-mlogloss:0.00278\n",
      "[364]\teval-mlogloss:0.35816\ttrain-mlogloss:0.00276\n",
      "[365]\teval-mlogloss:0.35818\ttrain-mlogloss:0.00275\n",
      "[366]\teval-mlogloss:0.35818\ttrain-mlogloss:0.00274\n",
      "[367]\teval-mlogloss:0.35822\ttrain-mlogloss:0.00273\n",
      "[368]\teval-mlogloss:0.35825\ttrain-mlogloss:0.00272\n",
      "[369]\teval-mlogloss:0.35827\ttrain-mlogloss:0.00270\n",
      "[370]\teval-mlogloss:0.35830\ttrain-mlogloss:0.00269\n",
      "[371]\teval-mlogloss:0.35835\ttrain-mlogloss:0.00268\n",
      "[372]\teval-mlogloss:0.35833\ttrain-mlogloss:0.00267\n",
      "[373]\teval-mlogloss:0.35837\ttrain-mlogloss:0.00266\n",
      "[374]\teval-mlogloss:0.35839\ttrain-mlogloss:0.00265\n",
      "[375]\teval-mlogloss:0.35839\ttrain-mlogloss:0.00263\n",
      "[376]\teval-mlogloss:0.35846\ttrain-mlogloss:0.00262\n",
      "[377]\teval-mlogloss:0.35847\ttrain-mlogloss:0.00260\n",
      "[378]\teval-mlogloss:0.35851\ttrain-mlogloss:0.00259\n",
      "[379]\teval-mlogloss:0.35861\ttrain-mlogloss:0.00258\n",
      "[380]\teval-mlogloss:0.35862\ttrain-mlogloss:0.00257\n",
      "[381]\teval-mlogloss:0.35871\ttrain-mlogloss:0.00256\n",
      "[382]\teval-mlogloss:0.35879\ttrain-mlogloss:0.00254\n",
      "[383]\teval-mlogloss:0.35883\ttrain-mlogloss:0.00253\n",
      "[384]\teval-mlogloss:0.35881\ttrain-mlogloss:0.00252\n",
      "[385]\teval-mlogloss:0.35882\ttrain-mlogloss:0.00251\n",
      "[386]\teval-mlogloss:0.35888\ttrain-mlogloss:0.00250\n",
      "[387]\teval-mlogloss:0.35896\ttrain-mlogloss:0.00248\n",
      "[388]\teval-mlogloss:0.35898\ttrain-mlogloss:0.00247\n",
      "[389]\teval-mlogloss:0.35902\ttrain-mlogloss:0.00246\n",
      "[390]\teval-mlogloss:0.35906\ttrain-mlogloss:0.00245\n",
      "[391]\teval-mlogloss:0.35904\ttrain-mlogloss:0.00244\n",
      "[392]\teval-mlogloss:0.35907\ttrain-mlogloss:0.00243\n",
      "[393]\teval-mlogloss:0.35908\ttrain-mlogloss:0.00242\n",
      "[394]\teval-mlogloss:0.35912\ttrain-mlogloss:0.00241\n",
      "[395]\teval-mlogloss:0.35917\ttrain-mlogloss:0.00240\n",
      "[396]\teval-mlogloss:0.35924\ttrain-mlogloss:0.00239\n",
      "[397]\teval-mlogloss:0.35931\ttrain-mlogloss:0.00237\n",
      "[398]\teval-mlogloss:0.35938\ttrain-mlogloss:0.00236\n",
      "[399]\teval-mlogloss:0.35942\ttrain-mlogloss:0.00235\n",
      "[400]\teval-mlogloss:0.35946\ttrain-mlogloss:0.00234\n",
      "[401]\teval-mlogloss:0.35948\ttrain-mlogloss:0.00233\n",
      "[402]\teval-mlogloss:0.35953\ttrain-mlogloss:0.00233\n",
      "[403]\teval-mlogloss:0.35957\ttrain-mlogloss:0.00232\n",
      "[404]\teval-mlogloss:0.35962\ttrain-mlogloss:0.00231\n",
      "[405]\teval-mlogloss:0.35966\ttrain-mlogloss:0.00230\n",
      "[406]\teval-mlogloss:0.35965\ttrain-mlogloss:0.00228\n",
      "[407]\teval-mlogloss:0.35974\ttrain-mlogloss:0.00228\n",
      "[408]\teval-mlogloss:0.35981\ttrain-mlogloss:0.00227\n",
      "[409]\teval-mlogloss:0.35984\ttrain-mlogloss:0.00226\n",
      "[410]\teval-mlogloss:0.35991\ttrain-mlogloss:0.00225\n",
      "[411]\teval-mlogloss:0.35997\ttrain-mlogloss:0.00224\n",
      "[412]\teval-mlogloss:0.35997\ttrain-mlogloss:0.00223\n",
      "[413]\teval-mlogloss:0.36001\ttrain-mlogloss:0.00222\n",
      "[414]\teval-mlogloss:0.36004\ttrain-mlogloss:0.00221\n",
      "[415]\teval-mlogloss:0.36007\ttrain-mlogloss:0.00220\n",
      "[416]\teval-mlogloss:0.36011\ttrain-mlogloss:0.00219\n",
      "[417]\teval-mlogloss:0.36017\ttrain-mlogloss:0.00218\n",
      "[418]\teval-mlogloss:0.36015\ttrain-mlogloss:0.00217\n",
      "[419]\teval-mlogloss:0.36018\ttrain-mlogloss:0.00216\n",
      "[420]\teval-mlogloss:0.36022\ttrain-mlogloss:0.00216\n",
      "[421]\teval-mlogloss:0.36028\ttrain-mlogloss:0.00215\n",
      "[422]\teval-mlogloss:0.36033\ttrain-mlogloss:0.00214\n",
      "[423]\teval-mlogloss:0.36038\ttrain-mlogloss:0.00213\n",
      "[424]\teval-mlogloss:0.36042\ttrain-mlogloss:0.00213\n",
      "[425]\teval-mlogloss:0.36049\ttrain-mlogloss:0.00212\n",
      "[426]\teval-mlogloss:0.36051\ttrain-mlogloss:0.00211\n",
      "[427]\teval-mlogloss:0.36057\ttrain-mlogloss:0.00210\n",
      "[428]\teval-mlogloss:0.36062\ttrain-mlogloss:0.00209\n",
      "[429]\teval-mlogloss:0.36068\ttrain-mlogloss:0.00209\n",
      "[430]\teval-mlogloss:0.36076\ttrain-mlogloss:0.00208\n",
      "[431]\teval-mlogloss:0.36083\ttrain-mlogloss:0.00207\n",
      "[432]\teval-mlogloss:0.36085\ttrain-mlogloss:0.00207\n",
      "[433]\teval-mlogloss:0.36091\ttrain-mlogloss:0.00205\n",
      "[434]\teval-mlogloss:0.36099\ttrain-mlogloss:0.00205\n",
      "[435]\teval-mlogloss:0.36101\ttrain-mlogloss:0.00204\n",
      "[436]\teval-mlogloss:0.36104\ttrain-mlogloss:0.00203\n",
      "[437]\teval-mlogloss:0.36107\ttrain-mlogloss:0.00202\n",
      "[438]\teval-mlogloss:0.36110\ttrain-mlogloss:0.00202\n",
      "[439]\teval-mlogloss:0.36115\ttrain-mlogloss:0.00201\n",
      "[440]\teval-mlogloss:0.36117\ttrain-mlogloss:0.00200\n",
      "[441]\teval-mlogloss:0.36126\ttrain-mlogloss:0.00200\n",
      "[442]\teval-mlogloss:0.36127\ttrain-mlogloss:0.00199\n",
      "[443]\teval-mlogloss:0.36127\ttrain-mlogloss:0.00198\n",
      "[444]\teval-mlogloss:0.36132\ttrain-mlogloss:0.00198\n",
      "[445]\teval-mlogloss:0.36136\ttrain-mlogloss:0.00197\n",
      "[446]\teval-mlogloss:0.36141\ttrain-mlogloss:0.00196\n",
      "[447]\teval-mlogloss:0.36144\ttrain-mlogloss:0.00196\n",
      "[448]\teval-mlogloss:0.36149\ttrain-mlogloss:0.00195\n",
      "[449]\teval-mlogloss:0.36155\ttrain-mlogloss:0.00195\n",
      "[450]\teval-mlogloss:0.36159\ttrain-mlogloss:0.00194\n",
      "[451]\teval-mlogloss:0.36157\ttrain-mlogloss:0.00193\n",
      "[452]\teval-mlogloss:0.36159\ttrain-mlogloss:0.00193\n",
      "[453]\teval-mlogloss:0.36160\ttrain-mlogloss:0.00192\n",
      "[454]\teval-mlogloss:0.36158\ttrain-mlogloss:0.00191\n",
      "[455]\teval-mlogloss:0.36160\ttrain-mlogloss:0.00191\n",
      "[456]\teval-mlogloss:0.36164\ttrain-mlogloss:0.00190\n",
      "[457]\teval-mlogloss:0.36169\ttrain-mlogloss:0.00190\n",
      "[458]\teval-mlogloss:0.36175\ttrain-mlogloss:0.00189\n",
      "[459]\teval-mlogloss:0.36174\ttrain-mlogloss:0.00188\n",
      "[460]\teval-mlogloss:0.36182\ttrain-mlogloss:0.00188\n",
      "[461]\teval-mlogloss:0.36190\ttrain-mlogloss:0.00187\n",
      "[462]\teval-mlogloss:0.36192\ttrain-mlogloss:0.00186\n",
      "[463]\teval-mlogloss:0.36198\ttrain-mlogloss:0.00186\n",
      "[464]\teval-mlogloss:0.36203\ttrain-mlogloss:0.00185\n",
      "[465]\teval-mlogloss:0.36207\ttrain-mlogloss:0.00185\n",
      "[466]\teval-mlogloss:0.36209\ttrain-mlogloss:0.00184\n",
      "[467]\teval-mlogloss:0.36217\ttrain-mlogloss:0.00184\n",
      "[468]\teval-mlogloss:0.36222\ttrain-mlogloss:0.00183\n",
      "[469]\teval-mlogloss:0.36228\ttrain-mlogloss:0.00182\n",
      "[470]\teval-mlogloss:0.36233\ttrain-mlogloss:0.00182\n",
      "[471]\teval-mlogloss:0.36237\ttrain-mlogloss:0.00181\n",
      "[472]\teval-mlogloss:0.36236\ttrain-mlogloss:0.00181\n",
      "[473]\teval-mlogloss:0.36245\ttrain-mlogloss:0.00180\n",
      "[474]\teval-mlogloss:0.36247\ttrain-mlogloss:0.00179\n",
      "[475]\teval-mlogloss:0.36248\ttrain-mlogloss:0.00179\n",
      "[476]\teval-mlogloss:0.36254\ttrain-mlogloss:0.00178\n",
      "[477]\teval-mlogloss:0.36260\ttrain-mlogloss:0.00178\n",
      "[478]\teval-mlogloss:0.36263\ttrain-mlogloss:0.00177\n",
      "[479]\teval-mlogloss:0.36266\ttrain-mlogloss:0.00177\n",
      "[480]\teval-mlogloss:0.36271\ttrain-mlogloss:0.00176\n",
      "[481]\teval-mlogloss:0.36276\ttrain-mlogloss:0.00176\n",
      "[482]\teval-mlogloss:0.36278\ttrain-mlogloss:0.00175\n",
      "[483]\teval-mlogloss:0.36284\ttrain-mlogloss:0.00175\n",
      "[484]\teval-mlogloss:0.36283\ttrain-mlogloss:0.00174\n",
      "[485]\teval-mlogloss:0.36285\ttrain-mlogloss:0.00174\n",
      "[486]\teval-mlogloss:0.36291\ttrain-mlogloss:0.00173\n",
      "[487]\teval-mlogloss:0.36291\ttrain-mlogloss:0.00173\n",
      "[488]\teval-mlogloss:0.36299\ttrain-mlogloss:0.00172\n",
      "[489]\teval-mlogloss:0.36302\ttrain-mlogloss:0.00172\n",
      "[490]\teval-mlogloss:0.36305\ttrain-mlogloss:0.00171\n",
      "[491]\teval-mlogloss:0.36306\ttrain-mlogloss:0.00171\n",
      "[492]\teval-mlogloss:0.36306\ttrain-mlogloss:0.00170\n",
      "[493]\teval-mlogloss:0.36313\ttrain-mlogloss:0.00170\n",
      "[494]\teval-mlogloss:0.36315\ttrain-mlogloss:0.00170\n",
      "[495]\teval-mlogloss:0.36318\ttrain-mlogloss:0.00169\n",
      "[496]\teval-mlogloss:0.36324\ttrain-mlogloss:0.00169\n",
      "[497]\teval-mlogloss:0.36334\ttrain-mlogloss:0.00168\n",
      "[498]\teval-mlogloss:0.36339\ttrain-mlogloss:0.00168\n",
      "[499]\teval-mlogloss:0.36344\ttrain-mlogloss:0.00167\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "dtrain = xgb.DMatrix(z_train, label=y_train)\n",
    "dval   = xgb.DMatrix(z_val,   label=y_val)\n",
    "dtest  = xgb.DMatrix(z_test_ordered)\n",
    "\n",
    "params = {\n",
    "    \"max_depth\": 6,\n",
    "    \"eta\": 0.1,\n",
    "    \"objective\": \"multi:softprob\",   # get probabilities\n",
    "    \"num_class\": len(np.unique(y_train)),\n",
    "    \"eval_metric\": \"mlogloss\",\n",
    "    \"tree_method\": \"hist\",\n",
    "}\n",
    "\n",
    "evallist = [(dval, \"eval\"), (dtrain, \"train\")]\n",
    "\n",
    "bst = xgb.train(\n",
    "    params,\n",
    "    dtrain,\n",
    "    num_boost_round=500,\n",
    "    evals=evallist,\n",
    "    early_stopping_rounds=20,\n",
    "    verbose_eval=True\n",
    ")\n",
    "\n",
    "posteriors_xgb = bst.predict(dtest)         # shape [N, C]\n",
    "y_pred_xgb = posteriors_xgb.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Info] Saved:\n",
      " - XGBoost model → xgb_model.json\n",
      " - posteriors_xgb.npy\n",
      " - y_pred_xgb.npy\n"
     ]
    }
   ],
   "source": [
    "np.save(f\"posteriors_xgb.npy\", posteriors_xgb)\n",
    "np.save(f\"y_pred_xgb.npy\", y_pred_xgb)\n",
    "\n",
    "# Save the model\n",
    "bst.save_model(f\"xgb_model.json\")\n",
    "\n",
    "print(f\"\\n[Info] Saved:\")\n",
    "print(f\" - XGBoost model → xgb_model.json\")\n",
    "print(f\" - posteriors_xgb.npy\")\n",
    "print(f\" - y_pred_xgb.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.1.1\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "print(xgb.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
